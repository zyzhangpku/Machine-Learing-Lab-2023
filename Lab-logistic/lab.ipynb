{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备工作\n",
    "### 环境准备\n",
    "\n",
    "请确保完成以下依赖包的安装，并且通过下面代码来导入与验证。运行成功后，你会看到一个新的窗口，其展示了一张空白的figure。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List\n",
    "\n",
    "# display the plot in a separate window\n",
    "%matplotlib tk\n",
    "\n",
    "np.random.seed(12)\n",
    "\n",
    "# create a figure and axis\n",
    "plt.ion()\n",
    "fig = plt.figure(figsize=(12, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集准备\n",
    "\n",
    "你将使用以下二维数据集来训练逻辑分类器，并观察随着训练的进行，线性分割面的变化。\n",
    "\n",
    "该数据集包含两个特征和一个标签，其中标签 $ y \\in \\{-1,1\\} $。\n",
    "\n",
    "请执行下面的代码以加载数据集并对其进行可视化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generator import gen_2D_dataset\n",
    "\n",
    "x_train, y_train = gen_2D_dataset(100, 100, noise = 0)\n",
    "x_test, y_test = gen_2D_dataset(50, 50, noise = 0.7) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vis_util import visualize_2D_dataset, visualize_2D_border\n",
    "\n",
    "visualize_2D_dataset(x_train, y_train)\n",
    "visualize_2D_dataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逻辑回归 (10 pts)\n",
    "\n",
    "在这一部分，你将学习并完成逻辑回归相关代码的编写与训练。\n",
    "\n",
    "在运行这部分代码之前，请确保你已经完成了 `logistics.py` 文件的代码补全。\n",
    "\n",
    "完成后，运行以下代码，你会看到一张figure来展示$||w||$，loss和决策边界的变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, loss: 136.14532709098182, w_module: 8.993435416843157\n",
      "iter: 10, loss: 0.359759630717664, w_module: 24.24719620628085\n",
      "iter: 20, loss: 0.13118174150957568, w_module: 24.278731117586574\n",
      "iter: 30, loss: 0.0809365439907071, w_module: 24.297910118626493\n",
      "iter: 40, loss: 0.05906695316217151, w_module: 24.31222741327293\n",
      "iter: 50, loss: 0.04700579115531036, w_module: 24.323904395326306\n",
      "iter: 60, loss: 0.03950744604858383, w_module: 24.33393739688583\n",
      "iter: 70, loss: 0.03450940749629354, w_module: 24.342868008663853\n",
      "iter: 80, loss: 0.03103076711442668, w_module: 24.351025905003333\n",
      "iter: 90, loss: 0.02854182134128911, w_module: 24.35862694836148\n",
      "iter: 100, loss: 0.026728971724398174, w_module: 24.365819526626513\n",
      "iter: 110, loss: 0.025393239126046276, w_module: 24.372709013530542\n",
      "iter: 120, loss: 0.024401547545706156, w_module: 24.379371839367078\n",
      "iter: 130, loss: 0.023661333032306053, w_module: 24.385864171657545\n",
      "iter: 140, loss: 0.023106373306861838, w_module: 24.3922275900966\n",
      "iter: 150, loss: 0.022688383985560147, w_module: 24.398492975547658\n",
      "iter: 160, loss: 0.022371746149622396, w_module: 24.404683274144677\n",
      "iter: 170, loss: 0.022130022602527565, w_module: 24.410815514075097\n",
      "iter: 180, loss: 0.021943549541041725, w_module: 24.416902302412314\n",
      "iter: 190, loss: 0.021797710366112954, w_module: 24.422952946699223\n",
      "iter: 200, loss: 0.021681665970476723, w_module: 24.428974298611855\n",
      "iter: 210, loss: 0.021587405600297527, w_module: 24.43497138850945\n",
      "iter: 220, loss: 0.021509031581002945, w_module: 24.44094790144535\n",
      "iter: 230, loss: 0.021442219089129785, w_module: 24.44690653278723\n",
      "iter: 240, loss: 0.021383808819225775, w_module: 24.452849252624507\n",
      "iter: 250, loss: 0.021331501082853845, w_module: 24.458777501383974\n",
      "iter: 260, loss: 0.021283627272805375, w_module: 24.46469233385819\n",
      "iter: 270, loss: 0.021238980084632535, w_module: 24.470594524781443\n",
      "iter: 280, loss: 0.021196688091033235, w_module: 24.47648464591341\n",
      "iter: 290, loss: 0.021156123568290334, w_module: 24.482363122126845\n",
      "iter: 300, loss: 0.021116835082454585, w_module: 24.48823027210032\n",
      "iter: 310, loss: 0.021078498393337374, w_module: 24.4940863377717\n",
      "iter: 320, loss: 0.021040880831086635, w_module: 24.499931505617333\n",
      "iter: 330, loss: 0.021003815530345277, w_module: 24.505765922004237\n",
      "iter: 340, loss: 0.02096718284429707, w_module: 24.511589704256266\n",
      "iter: 350, loss: 0.020930896967963075, w_module: 24.51740294862682\n",
      "iter: 360, loss: 0.020894896328627358, w_module: 24.523205736042648\n",
      "iter: 370, loss: 0.020859136692850222, w_module: 24.5289981362432\n",
      "iter: 380, loss: 0.020823586228097275, w_module: 24.53478021076612\n",
      "iter: 390, loss: 0.020788221968077954, w_module: 24.54055201510283\n",
      "iter: 400, loss: 0.020753027284592135, w_module: 24.546313600257605\n",
      "iter: 410, loss: 0.020717990080239307, w_module: 24.55206501387731\n",
      "iter: 420, loss: 0.020683101496929278, w_module: 24.557806301072198\n",
      "iter: 430, loss: 0.020648354993148578, w_module: 24.56353750501396\n",
      "iter: 440, loss: 0.02061374568478557, w_module: 24.569258667373155\n",
      "iter: 450, loss: 0.020579269874233017, w_module: 24.574969828640217\n",
      "iter: 460, loss: 0.020544924713914687, w_module: 24.580671028362442\n",
      "iter: 470, loss: 0.020510707965882696, w_module: 24.586362305319636\n",
      "iter: 480, loss: 0.020476617829968922, w_module: 24.592043697655264\n",
      "iter: 490, loss: 0.020442652820911778, w_module: 24.597715242974854\n",
      "iter: 500, loss: 0.02040881168047897, w_module: 24.603376978420602\n",
      "iter: 510, loss: 0.02037509331458833, w_module: 24.609028940728198\n",
      "iter: 520, loss: 0.020341496748292665, w_module: 24.614671166270586\n",
      "iter: 530, loss: 0.020308021093526724, w_module: 24.620303691091973\n",
      "iter: 540, loss: 0.02027466552601464, w_module: 24.625926550934533\n",
      "iter: 550, loss: 0.02024142926867973, w_module: 24.631539781259505\n",
      "iter: 560, loss: 0.02020831157976513, w_module: 24.637143417264195\n",
      "iter: 570, loss: 0.020175311744267274, w_module: 24.6427374938957\n",
      "iter: 580, loss: 0.020142429067815285, w_module: 24.648322045862177\n",
      "iter: 590, loss: 0.020109662872222434, w_module: 24.653897107642134\n",
      "iter: 600, loss: 0.020077012492308217, w_module: 24.659462713492285\n",
      "iter: 610, loss: 0.020044477273584323, w_module: 24.665018897454164\n",
      "iter: 620, loss: 0.02001205657057923, w_module: 24.67056569335979\n",
      "iter: 630, loss: 0.019979749745633175, w_module: 24.676103134836563\n",
      "iter: 640, loss: 0.019947556168001503, w_module: 24.68163125531156\n",
      "iter: 650, loss: 0.019915475213205035, w_module: 24.68715008801534\n",
      "iter: 660, loss: 0.019883506262550213, w_module: 24.692659665985303\n",
      "iter: 670, loss: 0.019851648702766657, w_module: 24.698160022068723\n",
      "iter: 680, loss: 0.019819901925743694, w_module: 24.703651188925516\n",
      "iter: 690, loss: 0.01978826532831327, w_module: 24.709133199030727\n",
      "iter: 700, loss: 0.019756738312093833, w_module: 24.714606084676902\n",
      "iter: 710, loss: 0.019725320283359715, w_module: 24.72006987797618\n",
      "iter: 720, loss: 0.019694010652944908, w_module: 24.72552461086235\n",
      "iter: 730, loss: 0.019662808836142957, w_module: 24.730970315092716\n",
      "iter: 740, loss: 0.019631714252651464, w_module: 24.7364070222499\n",
      "iter: 750, loss: 0.01960072632648856, w_module: 24.741834763743523\n",
      "iter: 760, loss: 0.019569844485956587, w_module: 24.747253570811846\n",
      "iter: 770, loss: 0.019539068163571614, w_module: 24.752663474523306\n",
      "iter: 780, loss: 0.019508396796026728, w_module: 24.75806450577804\n",
      "iter: 790, loss: 0.019477829824145372, w_module: 24.76345669530929\n",
      "iter: 800, loss: 0.019447366692833593, w_module: 24.76884007368484\n",
      "iter: 810, loss: 0.01941700685104278, w_module: 24.774214671308368\n",
      "iter: 820, loss: 0.019386749751732806, w_module: 24.779580518420744\n",
      "iter: 830, loss: 0.019356594851827114, w_module: 24.784937645101376\n",
      "iter: 840, loss: 0.019326541612176392, w_module: 24.79028608126943\n",
      "iter: 850, loss: 0.019296589497524975, w_module: 24.79562585668509\n",
      "iter: 860, loss: 0.019266737976472333, w_module: 24.800957000950767\n",
      "iter: 870, loss: 0.019236986521431628, w_module: 24.806279543512318\n",
      "iter: 880, loss: 0.01920733460860999, w_module: 24.811593513660178\n",
      "iter: 890, loss: 0.019177781717955172, w_module: 24.81689894053056\n",
      "iter: 900, loss: 0.01914832733313226, w_module: 24.822195853106564\n",
      "iter: 910, loss: 0.01911897094149302, w_module: 24.827484280219338\n",
      "iter: 920, loss: 0.019089712034027984, w_module: 24.83276425054913\n",
      "iter: 930, loss: 0.019060550105348902, w_module: 24.838035792626453\n",
      "iter: 940, loss: 0.01903148465364169, w_module: 24.84329893483311\n",
      "iter: 950, loss: 0.019002515180653552, w_module: 24.848553705403305\n",
      "iter: 960, loss: 0.01897364119163611, w_module: 24.85380013242467\n",
      "iter: 970, loss: 0.018944862195337954, w_module: 24.859038243839326\n",
      "iter: 980, loss: 0.018916177703959244, w_module: 24.864268067444918\n",
      "iter: 990, loss: 0.018887587233116912, w_module: 24.869489630895647\n"
     ]
    }
   ],
   "source": [
    "from logistic import LogisticRegression\n",
    "\n",
    "# create a LogisticRegression object \n",
    "LR = LogisticRegression()\n",
    "\n",
    "# fit the model to the training data without regularization (reg = 0)\n",
    "LR.fit(x_train, y_train, lr=0.1, n_iter=1000,reg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行上述代码，你会发现，在不考虑正则化的情况下，$||w||$ 随着训练次数的增加会不断增大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练完成后，你可以利用训练得到的分类器来进行预测。请你编写代码，计算训练集和测试集中的预测准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 100.0\n",
      "Test accuracy: 99.0\n"
     ]
    }
   ],
   "source": [
    "# Implement the code to compute the accuracy of logistic regression (LR) in the test set. Note that LR itself is already trained, if you have run the above code.\n",
    "\n",
    "# training accuracy\n",
    "def compute_acc(y_test, y_pred):\n",
    "    return (np.sum(y_test == y_pred) / y_test.shape[0]) * 100\n",
    "\n",
    "# TODO: compute the y_pred using LR.predict() function\n",
    "x = np.concatenate((x_train, np.ones((x_train.shape[0], 1))), axis=1)\n",
    "y_pred = LR.predict(x)[1]\n",
    "\n",
    "# TODO: compute the accuracy\n",
    "train_acc = compute_acc(y_train, y_pred)\n",
    "\n",
    "print(\"Train accuracy: {}\".format(train_acc))\n",
    "\n",
    "# TODO: test accuracy, proceed similarly as above\n",
    "x_test_modified = np.concatenate((x_test, np.ones((x_test.shape[0], 1))), axis=1)\n",
    "y_pred = LR.predict(x_test_modified)[1]\n",
    "\n",
    "test_acc = compute_acc(y_test, y_pred)\n",
    "\n",
    "print(\"Test accuracy: {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, loss: 152.45451735975882, w_module: 11.32850084425441\n",
      "iter: 10, loss: 15.229012221168228, w_module: 17.179193155454783\n",
      "iter: 20, loss: 12.717067631116134, w_module: 15.612012468648814\n",
      "iter: 30, loss: 10.806138783472308, w_module: 14.246365049809002\n",
      "iter: 40, loss: 9.41931685932985, w_module: 13.086407315557896\n",
      "iter: 50, loss: 8.480656088343297, w_module: 12.136250033768023\n",
      "iter: 60, loss: 7.899792694049424, w_module: 11.393157726481478\n",
      "iter: 70, loss: 7.575200935924055, w_module: 10.841599358152875\n",
      "iter: 80, loss: 7.411459160029551, w_module: 10.452937163736342\n",
      "iter: 90, loss: 7.336022845908454, w_module: 10.191309999841135\n",
      "iter: 100, loss: 7.303678380295568, w_module: 10.021462596463838\n",
      "iter: 110, loss: 7.290506041904443, w_module: 9.914067060190575\n",
      "iter: 120, loss: 7.28531856668592, w_module: 9.847373376301473\n",
      "iter: 130, loss: 7.283314996444741, w_module: 9.806442507988576\n",
      "iter: 140, loss: 7.282548008302744, w_module: 9.781511476891872\n",
      "iter: 150, loss: 7.282254694905058, w_module: 9.766397929067349\n",
      "iter: 160, loss: 7.282141951447905, w_module: 9.757263130927921\n",
      "iter: 170, loss: 7.282098174993593, w_module: 9.751752318259454\n",
      "iter: 180, loss: 7.282080931563369, w_module: 9.7484317772304\n",
      "iter: 190, loss: 7.282074016545705, w_module: 9.746432583231739\n",
      "iter: 200, loss: 7.282071185382961, w_module: 9.745229603546836\n",
      "iter: 210, loss: 7.282069999801644, w_module: 9.744506036904545\n",
      "iter: 220, loss: 7.282069491651788, w_module: 9.74407097889051\n",
      "iter: 230, loss: 7.282069268840859, w_module: 9.743809475328966\n",
      "iter: 240, loss: 7.282069169046063, w_module: 9.743652340993687\n",
      "iter: 250, loss: 7.282069123492371, w_module: 9.743557952264634\n",
      "iter: 260, loss: 7.282069102356215, w_module: 9.743501274807077\n",
      "iter: 270, loss: 7.282069092415425, w_module: 9.743467255824807\n",
      "iter: 280, loss: 7.282069087688463, w_module: 9.743446846592075\n",
      "iter: 290, loss: 7.282069085421139, w_module: 9.743434609057992\n",
      "iter: 300, loss: 7.28206908432624, w_module: 9.74342727602571\n",
      "iter: 310, loss: 7.282069083794765, w_module: 9.743422885178306\n",
      "iter: 320, loss: 7.282069083535772, w_module: 9.743420258351767\n",
      "iter: 330, loss: 7.282069083409187, w_module: 9.743418688477895\n",
      "iter: 340, loss: 7.282069083347181, w_module: 9.743417751419086\n",
      "iter: 350, loss: 7.28206908331676, w_module: 9.7434171928982\n",
      "iter: 360, loss: 7.282069083301814, w_module: 9.74341686057271\n",
      "iter: 370, loss: 7.282069083294467, w_module: 9.743416663241831\n",
      "iter: 380, loss: 7.2820690832908515, w_module: 9.743416546357412\n",
      "iter: 390, loss: 7.282069083289073, w_module: 9.7434164773287\n",
      "iter: 400, loss: 7.282069083288198, w_module: 9.743416436708522\n",
      "iter: 410, loss: 7.282069083287764, w_module: 9.743416412910078\n",
      "iter: 420, loss: 7.282069083287551, w_module: 9.74341639904223\n",
      "iter: 430, loss: 7.282069083287447, w_module: 9.743416391015323\n",
      "iter: 440, loss: 7.282069083287395, w_module: 9.743416386408514\n",
      "iter: 450, loss: 7.282069083287368, w_module: 9.74341638379324\n",
      "iter: 460, loss: 7.282069083287357, w_module: 9.743416382329665\n",
      "iter: 470, loss: 7.28206908328735, w_module: 9.743416381526316\n",
      "iter: 480, loss: 7.282069083287347, w_module: 9.743416381097223\n",
      "iter: 490, loss: 7.282069083287347, w_module: 9.743416380877159\n",
      "iter: 500, loss: 7.282069083287346, w_module: 9.743416380771519\n",
      "iter: 510, loss: 7.282069083287346, w_module: 9.743416380726755\n",
      "iter: 520, loss: 7.282069083287347, w_module: 9.743416380713022\n",
      "iter: 530, loss: 7.282069083287345, w_module: 9.74341638071403\n",
      "iter: 540, loss: 7.282069083287347, w_module: 9.743416380721149\n",
      "iter: 550, loss: 7.282069083287345, w_module: 9.74341638073\n",
      "iter: 560, loss: 7.282069083287346, w_module: 9.743416380738537\n",
      "iter: 570, loss: 7.282069083287345, w_module: 9.743416380745929\n",
      "iter: 580, loss: 7.282069083287346, w_module: 9.74341638075196\n",
      "iter: 590, loss: 7.282069083287347, w_module: 9.743416380756695\n",
      "iter: 600, loss: 7.282069083287346, w_module: 9.743416380760326\n",
      "iter: 610, loss: 7.282069083287347, w_module: 9.743416380763056\n",
      "iter: 620, loss: 7.282069083287347, w_module: 9.743416380765085\n",
      "iter: 630, loss: 7.282069083287347, w_module: 9.743416380766575\n",
      "iter: 640, loss: 7.282069083287345, w_module: 9.74341638076766\n",
      "iter: 650, loss: 7.282069083287346, w_module: 9.743416380768448\n",
      "iter: 660, loss: 7.282069083287345, w_module: 9.743416380769016\n",
      "iter: 670, loss: 7.282069083287345, w_module: 9.743416380769421\n",
      "iter: 680, loss: 7.282069083287345, w_module: 9.743416380769714\n",
      "iter: 690, loss: 7.282069083287347, w_module: 9.743416380769926\n",
      "iter: 700, loss: 7.282069083287345, w_module: 9.743416380770071\n",
      "iter: 710, loss: 7.282069083287343, w_module: 9.743416380770174\n",
      "iter: 720, loss: 7.282069083287345, w_module: 9.743416380770247\n",
      "iter: 730, loss: 7.282069083287347, w_module: 9.7434163807703\n",
      "iter: 740, loss: 7.282069083287347, w_module: 9.743416380770338\n",
      "iter: 750, loss: 7.282069083287345, w_module: 9.743416380770366\n",
      "iter: 760, loss: 7.282069083287345, w_module: 9.743416380770384\n",
      "iter: 770, loss: 7.282069083287345, w_module: 9.7434163807704\n",
      "iter: 780, loss: 7.282069083287345, w_module: 9.743416380770407\n",
      "iter: 790, loss: 7.282069083287345, w_module: 9.743416380770412\n",
      "iter: 800, loss: 7.282069083287347, w_module: 9.743416380770412\n",
      "iter: 810, loss: 7.282069083287346, w_module: 9.743416380770412\n",
      "iter: 820, loss: 7.282069083287345, w_module: 9.743416380770412\n",
      "iter: 830, loss: 7.282069083287345, w_module: 9.743416380770418\n",
      "iter: 840, loss: 7.282069083287345, w_module: 9.743416380770421\n",
      "iter: 850, loss: 7.282069083287347, w_module: 9.74341638077042\n",
      "iter: 860, loss: 7.282069083287347, w_module: 9.74341638077042\n",
      "iter: 870, loss: 7.282069083287345, w_module: 9.74341638077042\n",
      "iter: 880, loss: 7.282069083287346, w_module: 9.74341638077042\n",
      "iter: 890, loss: 7.282069083287346, w_module: 9.74341638077042\n",
      "iter: 900, loss: 7.282069083287347, w_module: 9.74341638077042\n",
      "iter: 910, loss: 7.282069083287347, w_module: 9.74341638077042\n",
      "iter: 920, loss: 7.282069083287348, w_module: 9.74341638077042\n",
      "iter: 930, loss: 7.282069083287348, w_module: 9.743416380770421\n",
      "iter: 940, loss: 7.282069083287347, w_module: 9.743416380770421\n",
      "iter: 950, loss: 7.282069083287347, w_module: 9.743416380770421\n",
      "iter: 960, loss: 7.282069083287347, w_module: 9.743416380770421\n",
      "iter: 970, loss: 7.282069083287347, w_module: 9.743416380770421\n",
      "iter: 980, loss: 7.282069083287347, w_module: 9.743416380770421\n",
      "iter: 990, loss: 7.282069083287347, w_module: 9.743416380770421\n"
     ]
    }
   ],
   "source": [
    "# create a LogisticRegression object and train it when using regularization\n",
    "LR = LogisticRegression()\n",
    "LR.fit(x_train, y_train, lr=0.1, n_iter=1000,reg=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 100.0\n",
      "Test accuracy: 99.0\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement the code to compute the accuracy of logistic regression (LR) in the test set. Note that LR itself is already trained, if you have run the above code.\n",
    "\n",
    "# TODO: compute the y_pred using LR.predict() function\n",
    "x = np.concatenate((x_train, np.ones((x_train.shape[0], 1))), axis=1)\n",
    "y_pred = LR.predict(x)[1]\n",
    "\n",
    "# TODO: compute the accuracy\n",
    "train_acc = compute_acc(y_train, y_pred)\n",
    "\n",
    "print(\"Train accuracy: {}\".format(train_acc))\n",
    "\n",
    "# TODO: test accuracy, proceed similarly as above\n",
    "x_test_modified = np.concatenate((x_test, np.ones((x_test.shape[0], 1))), axis=1)\n",
    "y_pred = LR.predict(x_test_modified)[1]\n",
    "\n",
    "test_acc = compute_acc(y_test, y_pred)\n",
    "\n",
    "print(\"Test accuracy: {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行上述带有正则化的代码后，请观察 $||w||$ 的变化，并讨论正则化的实际意义。(请将答案写在下方)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$||w||$ 随着迭代次数的增加逐渐下降，可以防止过拟合，提高模型的泛化能力，防止某些特征获得很大的权重，降低variance。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
