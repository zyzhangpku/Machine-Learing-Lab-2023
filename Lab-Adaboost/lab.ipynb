{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备工作\n",
    "### 环境准备\n",
    "请确保完成以下依赖包的安装，并且通过下面代码来导入与验证。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集准备\n",
    "我们将使用以下数据集进行 Adaboost 的训练。\n",
    "\n",
    "该数据集与决策树部分使用的数据集相同，包括 7 个特征以及一个标签“是否适合攻读博士”，涵盖了适合攻读博士的各种条件，如love doing research,I absolutely want to be a college professor等。\n",
    "\n",
    "请执行下面的代码来加载数据集。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read decision_tree_datasets.csv\n",
    "train_data = pd.read_csv('train_phd_data.csv')\n",
    "test_data = pd.read_csv('test_phd_data.csv')\n",
    "\n",
    "# translate lables [0,1] to [-1,1]\n",
    "# if 0 then -1, if 1 then 1\n",
    "train_data.iloc[:, -1] = train_data.iloc[:, -1].map({0: -1, 1: 1})\n",
    "test_data.iloc[:, -1] = test_data.iloc[:, -1].map({0: -1, 1: 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost (15 pts)\n",
    "\n",
    "在上一个lab中，你已经成功完成了 Decision Tree 的构建。在本部分，你可以继续沿用上一部分的代码，学习并完成 Adaboost 模型的训练。\n",
    "\n",
    "在这个 Adaboost 模型中，我们选择了一层决策树作为弱学习器，并使用基尼系数作为分类标准。\n",
    "\n",
    "请完成以下类的构建以及相应函数的实现：\n",
    "\n",
    "1. **weakClassifier()**: 我们采用一层决策树，包括 `split()` 和 `predict()`。你可以参考上一次实验中的代码。\n",
    "\n",
    "2. **Adaboost()** ：包括弱学习器的集合，拟合过程 `fit()` 和预测过程 `predict()`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class weakClassifier:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.tree = None\n",
    "        self.alpha = None\n",
    "\n",
    "    @staticmethod\n",
    "    def split_data(data, column):\n",
    "        # the same as in the tree lab\n",
    "\n",
    "        splt_datas = pd.Series(dtype='float64')\n",
    "        str_values = data.iloc[:, column].unique()\n",
    "        for i in range(len(str_values)):\n",
    "            df = data.loc[data.iloc[:, column] == str_values[i]]\n",
    "            splt_datas[str(i)] = df\n",
    "        return splt_datas\n",
    "\n",
    "    def best_split(self, X, y, sample_weight):\n",
    "\n",
    "        best_feature_index = 0\n",
    "        numFeatures = X.shape[1]\n",
    "        \n",
    "        # to be same as the tree lab, add labels\n",
    "        X['label'] = y\n",
    "        X['SampleWeight'] = sample_weight\n",
    "\n",
    "        best_gini = 100\n",
    "        best_Series = self.split_data(X, 0)\n",
    "\n",
    "        for i in range(numFeatures):\n",
    "            gini = 1\n",
    "            series = self.split_data(X, i)\n",
    "\n",
    "            for j in range(len(series)):\n",
    "                df = series.iloc[j]\n",
    "                p1, p2 = np.sum(df[df.iloc[:, -2] == 1].iloc[:, -1]), np.sum(df[df.iloc[:, -2] == -1].iloc[:, -1])\n",
    "                gini -= df.shape[0] / X.shape[0] / np.sum(df.iloc[:, -1]) ** 2 * (p1 ** 2 + p2 ** 2)\n",
    "\n",
    "            if gini < best_gini:\n",
    "                best_gini = gini\n",
    "                best_feature_index = i\n",
    "                best_Series = series\n",
    "\n",
    "        return X.columns[best_feature_index], best_Series\n",
    "\n",
    "    def fit(self, X, y, sample_weight):\n",
    "        \"\"\"\n",
    "            fit the data to the decision tree\n",
    "\n",
    "            Args:\n",
    "                X: the features of the data\n",
    "                y: the labels of the data\n",
    "                sample_weight: the weight of each sample\n",
    "\n",
    "            Returns:\n",
    "                None, but self.tree should be updated\n",
    "        \"\"\"\n",
    "        best_feature, best_splits = self.best_split(X, y, sample_weight)\n",
    "\n",
    "        if best_feature is None:\n",
    "            return\n",
    "        tree = {best_feature: {}}\n",
    "        for df in best_splits:\n",
    "            prediction = df.loc[:, best_feature].unique()[0]\n",
    "\n",
    "            if np.sum(df.iloc[:, -2] * df.iloc[:, -1]) > 0:\n",
    "                tree[best_feature][prediction] = 1\n",
    "            else:\n",
    "                tree[best_feature][prediction] = -1\n",
    "        self.tree = tree\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        predict the label of the data\n",
    "\n",
    "        Args:\n",
    "            x: the features of the data\n",
    "        Return:\n",
    "            predict_labels: the predict labels of the data\n",
    "        \"\"\"\n",
    "\n",
    "        predict_labels = []\n",
    "\n",
    "        for i in range(len(x)):\n",
    "            sample = x.iloc[i, :]\n",
    "            first_str = list(self.tree.keys())[0]\n",
    "            feat_index = sample.index.get_loc(first_str)\n",
    "            key = sample.iloc[feat_index]\n",
    "            predict_labels.append(self.tree[first_str][key])\n",
    "\n",
    "        return predict_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaboost:\n",
    "\n",
    "    def __init__(self, n_estimators=10):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.clfs = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, m_features = X.shape\n",
    "        w = np.ones(n_samples) / n_samples\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            clf = weakClassifier()\n",
    "            clf.fit(X, y, w)\n",
    "            X.drop([\"label\", \"SampleWeight\"], axis=1, inplace=True)\n",
    "            \n",
    "            y_pred = clf.predict(X)\n",
    "            error = np.sum(w * np.where(y_pred == y, 0, 1))\n",
    "            alpha = np.log((1 - error) / error) / 2\n",
    "\n",
    "            w *= np.exp(-alpha * y * y_pred)\n",
    "            w /= np.sum(w)\n",
    "\n",
    "            # save classifier and weight\n",
    "            clf.alpha = alpha\n",
    "            self.clfs.append(clf)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        predict the label of the data\n",
    "\n",
    "        Args:\n",
    "            X: the features of the data\n",
    "        Return:\n",
    "            y_pred: the predicted labels of the data\n",
    "        \"\"\"\n",
    "        y_pred, alpha = [], []\n",
    "\n",
    "        for clf in self.clfs:\n",
    "            y_pred.append(clf.predict(X))\n",
    "            alpha.append(clf.alpha)\n",
    "\n",
    "        w_sum = np.dot(np.array(alpha), np.array(y_pred))\n",
    "\n",
    "        return np.sign(w_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of Adaboost is:  1.0\n"
     ]
    }
   ],
   "source": [
    "adaboost_model = Adaboost(n_estimators=10)\n",
    "adaboost_model.fit(train_data.iloc[:, :-1], train_data.iloc[:, -1])\n",
    "predictions = adaboost_model.predict(test_data.iloc[:, :-1])\n",
    "accuracy = np.mean(predictions == test_data.iloc[:, -1].values)\n",
    "print(\"The accuracy of Adaboost is: \", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
