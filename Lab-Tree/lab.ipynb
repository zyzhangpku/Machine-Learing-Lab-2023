{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备工作\n",
    "### 环境准备\n",
    "请确保完成以下依赖包的安装，并且通过下面代码来导入与验证。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集准备\n",
    "我们将使用以下数据集进行决策树的构建。该数据集包括7个特征，以及一个标签“是否适合读博”，这些特征描述了适合读博的各种条件，如love doing research,I absolutely want to be a college professor等。\n",
    "\n",
    "请运行下面的代码来加载数据集。\n",
    "\n",
    "（防侵权说明）参考https://zhuanlan.zhihu.com/p/372884253，数据集来源GPT4，但构造的决策树不一定与参考内容完全一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read decision_tree_datasets.csv\n",
    "train_data = pd.read_csv('train_phd_data.csv')\n",
    "test_data = pd.read_csv('test_phd_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树构建 (10 分)\n",
    "在这个部分，你将学习并完成决策树的构建。注意：不考虑剪枝，决策树构建停止条件是数据所有实例属于同一类或者特征不可再分（即每个特征值都一样）。\n",
    "\n",
    "我们采用信息增益率作为分类标准，同时也允许使用其他指标，如基尼系数。\n",
    "\n",
    "请完成以下函数：\n",
    "\n",
    "1. **计算数据的信息熵** `getInfoEntropy()`\n",
    "2. **根据选取的特征进行数据分割** `split_data()`\n",
    "3. **根据分类标准找到最优特征** `find_best_feature()`\n",
    "\n",
    "你可能会用到`pandas`库函数，请参考 [pandas官方文档](https://pandas.pydata.org/docs/reference/)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInfoEntropy(data):\n",
    "    \"\"\"\n",
    "        calculate the information entropy of the data\n",
    "\n",
    "    Args:\n",
    "        data: the data set, the last column is the label, the other columns are the features\n",
    "\n",
    "    Returns:\n",
    "        Entropy: float, the information entropy of the data\n",
    "    \"\"\"\n",
    "\n",
    "    Entropy = 0.0\n",
    "\n",
    "    count_class = data.iloc[:, -1].value_counts()\n",
    "\n",
    "    data_count = count_class.sum()\n",
    "\n",
    "    for count in count_class:\n",
    "        p = count / data_count\n",
    "        Entropy += -p * np.log2(p)\n",
    "    return Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8256265261578954\n"
     ]
    }
   ],
   "source": [
    "## test getInfoEntropy\n",
    "print(getInfoEntropy(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, column):\n",
    "    \"\"\"\n",
    "        split the data set according to the feature column\n",
    "\n",
    "        Args:\n",
    "            data: the data set, the last column is the label, the other columns are the features\n",
    "            column: the feature column\n",
    "        Returns:\n",
    "            splt_datas: Series, the data set after splitting\n",
    "    \"\"\"\n",
    "    # 1. construct a Series to save the data set after splitting\n",
    "    splt_datas = pd.Series(dtype='float64')\n",
    "    # 2. get the unique values of the feature column\n",
    "    str_values = data.iloc[:, column].unique()\n",
    "    # 3. find the data set corresponding to each unique value\n",
    "    for i in range(len(str_values)):\n",
    "        df = data.loc[data.iloc[:, column] == str_values[i]]\n",
    "        splt_datas[str(i)] = df\n",
    "    return splt_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_feature(data):\n",
    "    \"\"\"\n",
    "        find the best feature to split the data set\n",
    "\n",
    "        Args:\n",
    "            data: the data set, the last column is the label, the other columns are the features\n",
    "        Returns:\n",
    "            best_feature: the best feature\n",
    "            best_Series: Series, the data set after splitting\n",
    "    \"\"\"\n",
    "    best_feature_index = 0\n",
    "    baseEnt = getInfoEntropy(data)\n",
    "    bestInfoGain_ratio = 0.0\n",
    "    numFeatures = data.shape[1] - 1\n",
    "    numData = data.shape[0]\n",
    "\n",
    "    # Loop through each feature to calculate information gain ratio.\n",
    "    for i in range(numFeatures):\n",
    "        newEnt = 0.0\n",
    "        # avoid div 0 error\n",
    "        IV = 1e-5\n",
    "        # 1. split the data set according to the feature column\n",
    "        series = split_data(data, i)\n",
    "\n",
    "        # 2. calculate the information entropy of each data set, and calculate the weighted average information entropy\n",
    "        for j in range(len(series)):\n",
    "            df = series[j]\n",
    "            # 3. calculate the probability of each data set\n",
    "            p = df.shape[0] / numData\n",
    "            # 4. calculate the weighted average information entropy\n",
    "            newEnt += p * getInfoEntropy(df)\n",
    "            # 5. calculate the entropy of class labels IV\n",
    "            IV += -p * np.log2(p)\n",
    "\n",
    "        # 6. calculate the information gain\n",
    "        InfoGain = baseEnt - newEnt\n",
    "\n",
    "        # 7. calculate the information gain ratio\n",
    "        InfoGain_ratio = InfoGain / IV\n",
    "\n",
    "        if InfoGain_ratio > bestInfoGain_ratio:\n",
    "            bestInfoGain_ratio = InfoGain_ratio\n",
    "            best_feature_index = i\n",
    "            best_Series = series\n",
    "\n",
    "    return data.columns[best_feature_index], best_Series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Tree(data):\n",
    "    \"\"\"\n",
    "        create decision tree\n",
    "\n",
    "        Args:\n",
    "            data: the data set, the last column is the label, the other columns are the features\n",
    "        Returns:\n",
    "            Tree: dict, the decision tree\n",
    "    \"\"\"\n",
    "    # get the class labels of the data set\n",
    "    y_values = data.iloc[:, -1].unique()\n",
    "\n",
    "    # TODO: 1. If there is only one class label, stop splitting and return the class label.\n",
    "    if len(y_values) == 1:\n",
    "        return y_values[0]\n",
    "\n",
    "    # 2. Check if the value of each feature is the same. If so, return the class label with the most samples.\n",
    "    flag = 0\n",
    "    for i in range(data.shape[1] - 1):\n",
    "        if len(data.iloc[:, i].unique()) != 1:\n",
    "            flag = 1\n",
    "            break\n",
    "\n",
    "    # TODO: 3. If all features are identical, return the class label with the most samples.\n",
    "    if flag == 0:\n",
    "        return data.iloc[:, -1].value_counts().idxmax()\n",
    "\n",
    "    # 4. Find the best feature to split the data set.\n",
    "    best_feature, best_Series = find_best_feature(data)\n",
    "    Tree = {best_feature: {}}\n",
    "    # 5. Build the tree recursively.\n",
    "    for j in range(len(best_Series)):\n",
    "        split_data = best_Series.iloc[j]\n",
    "\n",
    "        # read the value of the best feature\n",
    "        value = split_data.loc[:, best_feature].unique()[0]\n",
    "\n",
    "        # delete the best feature\n",
    "        split_data = split_data.drop(best_feature, axis=1)\n",
    "\n",
    "        # TODO: 6. recursively call the function to build the tree\n",
    "        Tree[best_feature][value] = create_Tree(split_data)\n",
    "\n",
    "    return Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I absolutely want to be a college professor': {0: {'I am OK being with judged all the time': {0: 0, 1: {'I work 9-5 Mon-Fri': {1: 0, 0: {'I need a clear target and immediate feedback': {0: 1, 1: 0}}}}}}, 1: {'I love doing research': {1: {'I am OK being with judged all the time': {1: 1, 0: {'I can deal with extreme stress and competition': {1: 0, 0: 1}}}}, 0: 0}}}}\n"
     ]
    }
   ],
   "source": [
    "Tree = create_Tree(train_data)\n",
    "print(Tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'decision_tree.png'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize decision tree\n",
    "from graphviz import Digraph\n",
    "\n",
    "\n",
    "def plot_tree(tree, parent_name, node_id=0, graph=None, edge_label=''):\n",
    "    \n",
    "    \n",
    "    if graph is None:\n",
    "        graph = Digraph(comment='Decision Tree')\n",
    "\n",
    "    \n",
    "    if not isinstance(tree, dict):\n",
    "        current_node_name = f'node{node_id}' \n",
    "        graph.node(current_node_name, label=str(tree))\n",
    "        graph.edge(parent_name, current_node_name, label=edge_label)\n",
    "        node_id += 1\n",
    "        return node_id\n",
    "    \n",
    "    for k, v in tree.items():\n",
    "        current_node_name = f'node{node_id}' \n",
    "        node_label = f'{k}' if isinstance(v, (str, int)) else k\n",
    "        graph.node(current_node_name, label=node_label)\n",
    "        graph.edge(parent_name, current_node_name, label=str(edge_label))\n",
    "\n",
    "        if isinstance(v, dict):\n",
    "            for key in v:\n",
    "                # 假设分支可以用 '0' 和 '1' 来区分\n",
    "                node_id += 1\n",
    "                node_id = plot_tree(v[key], current_node_name, node_id, graph, edge_label=str(key))\n",
    "                \n",
    "    return node_id\n",
    "\n",
    "# plot decision tree\n",
    "tree_graph = Digraph(comment='Decision Tree')\n",
    "plot_tree(Tree, 'Root', 0, tree_graph)\n",
    "tree_graph.render('decision_tree', format='png', cleanup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classfiy test data\n",
    "def classify(tree, test_data):\n",
    "    ''' \n",
    "        classify test data\n",
    "\n",
    "        Args:\n",
    "            tree: dict, the decision tree\n",
    "            test_data: the test data set, the last column is the label, the other columns are the features\n",
    "        Returns:\n",
    "            class_label: the predicted class label\n",
    "    '''\n",
    "    ## get the checked feature of the decision tree\n",
    "    first_str = list(tree.keys())[0]\n",
    "    \n",
    "    ## get the index of the feature\n",
    "    feat_index = test_data.index.get_loc(first_str)\n",
    "\n",
    "    # get the value of the feature to be checked in the decision tree\n",
    "    key = test_data.iloc[feat_index]\n",
    "\n",
    "    # get the subtree corresponding to the value of the feature\n",
    "    subtree = tree[first_str][key]\n",
    "\n",
    "    # Check if the subtree is a dictionary (not a leaf node)\n",
    "    if isinstance(subtree, dict):\n",
    "        # Recursively call the function to classify the test data using the subtree\n",
    "        class_label = classify(subtree, test_data)\n",
    "    else:\n",
    "        # The subtree is a leaf node, and its value is the predicted class label\n",
    "        class_label = subtree\n",
    "\n",
    "    return class_label\n",
    "\n",
    "\n",
    "def test(test_data, tree):\n",
    "    right_label = []\n",
    "    for i in range(len(test_data)):\n",
    "        sample = test_data.iloc[i]\n",
    "        \n",
    "        # whether the predicted class label is equal to the true class label\n",
    "        if classify(tree, sample) == sample[-1]:\n",
    "            # if equal, the predicted is 1\n",
    "            right_label.append(1)\n",
    "        else:\n",
    "            right_label.append(0)\n",
    "    \n",
    "    acc = sum(right_label) / len(right_label)\n",
    "    print('accuracy: ', acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.8571428571428571\n",
      "accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "test(test_data, Tree)\n",
    "test(train_data, Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们将进行一项小测试，目的在于评估您是否适合攻读博士学位。\n",
    "\n",
    "请注意！这仅仅是基于假设的模型，无法准确预测实际情况。请将其视为一次轻松的尝试，仅供娱乐之用，不要用其来替代对自身状况的思考与决策。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you love research? (1/0)1\n",
      "Do you want to be a professor? (1/0)0\n",
      "Is money important to you? (1/0)0\n",
      "Can you deal with stress? (1/0)1\n",
      "Can you deal with being judged all the time? (1/0)1\n",
      "Do you need a clear target and immediate feedback? (1/0)0\n",
      "Do you work 9-5 Mon-Fri? (1/0)0\n",
      "Congratulations! According to the model, you are likely to gain admission for Ph.D.\n"
     ]
    }
   ],
   "source": [
    "# You can input your profile to predict your phd admission result\n",
    "# input your profile about \"I love doing research,I absolutely want to be a college professor,Money is important to me,I can deal with extreme stress and competition,I am OK being with judged all the time,I need a clear target and immediate feedback,I work 9-5 Mon-Fri\"\n",
    "loving = input('Do you love research? (1/0)')\n",
    "professor = input('Do you want to be a professor? (1/0)')\n",
    "money = input('Is money important to you? (1/0)')\n",
    "stress = input('Can you deal with stress? (1/0)')\n",
    "judge = input('Can you deal with being judged all the time? (1/0)')\n",
    "feedback = input('Do you need a clear target and immediate feedback? (1/0)')\n",
    "work = input('Do you work 9-5 Mon-Fri? (1/0)')\n",
    "\n",
    "# Combine the user's responses into a single data frame\n",
    "test_data = pd.Series({\n",
    "    'I love doing research': int(loving),\n",
    "    'I absolutely want to be a college professor': int(professor),\n",
    "    'Money is important to me': int(money),\n",
    "    'I can deal with extreme stress and competition': int(stress),\n",
    "    'I am OK being with judged all the time': int(judge),\n",
    "    'I need a clear target and immediate feedback': int(feedback),\n",
    "    'I work 9-5 Mon-Fri': int(work)\n",
    "})\n",
    "\n",
    "# Use the decision tree to predict the result \n",
    "result = classify(Tree, test_data)\n",
    "\n",
    "# Print the result to the user\n",
    "if result == 1:\n",
    "    print(\"Congratulations! According to the model, you are likely to gain admission for Ph.D.\")\n",
    "elif result == 0: \n",
    "    print(\"Unfortunately, according to the model, you are unlikely to gain admission for Ph.D.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
